{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd4d92-0c79-4af5-b557-c4f719b4346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "from pytrends.request import TrendReq\n",
    "from openpyxl import Workbook\n",
    "import time\n",
    "\n",
    "# -------------------------\n",
    "# STEP 1: SCRAPE WEBSITE\n",
    "# -------------------------\n",
    "def scrape_seo_elements(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    title = soup.title.string if soup.title else \"\"\n",
    "    meta_desc = soup.find(\"meta\", {\"name\": \"description\"})\n",
    "    meta_desc = meta_desc[\"content\"] if meta_desc else \"\"\n",
    "\n",
    "    headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3'])]\n",
    "    body_content = \" \".join([p.get_text(strip=True) for p in soup.find_all('p')])\n",
    "\n",
    "    # Blog Tags (Assume tags have <a rel='tag'> or 'tag' in class)\n",
    "    tags = [tag.get_text(strip=True) for tag in soup.find_all(\"a\", {\"rel\": \"tag\"})]\n",
    "    if not tags:\n",
    "        tags = [tag.get_text(strip=True) for tag in soup.find_all(\"a\", class_=re.compile(\"tag\"))]\n",
    "\n",
    "    internal_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if url.split(\"/\")[2] in a[\"href\"]]\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"meta_desc\": meta_desc,\n",
    "        \"headings\": headings,\n",
    "        \"body_content\": body_content,\n",
    "        \"tags\": tags,\n",
    "        \"internal_links\": internal_links\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# STEP 2: KEYWORD EXTRACTION\n",
    "# -------------------------\n",
    "def extract_keywords(text, min_keywords=300):\n",
    "    r = Rake(stopwords=stopwords.words('english'))\n",
    "    r.extract_keywords_from_text(text)\n",
    "    phrases = r.get_ranked_phrases()\n",
    "\n",
    "    # Ensure at least 300 keywords (repeat or trim)\n",
    "    if len(phrases) < min_keywords:\n",
    "        phrases = phrases * (min_keywords // len(phrases) + 1)\n",
    "    phrases = list(dict.fromkeys(phrases))[:min_keywords]  # remove duplicates & trim\n",
    "\n",
    "    # Classify into short, mid, long tail\n",
    "    short_tail, mid_tail, long_tail = [], [], []\n",
    "    for kw in phrases:\n",
    "        wc = len(kw.split())\n",
    "        if wc == 2:\n",
    "            short_tail.append(kw)\n",
    "        elif wc == 3:\n",
    "            mid_tail.append(kw)\n",
    "        elif wc >= 4:\n",
    "            long_tail.append(kw)\n",
    "\n",
    "    return short_tail, mid_tail, long_tail\n",
    "\n",
    "# -------------------------\n",
    "# STEP 3: PYTRENDS KEYWORD ANALYSIS\n",
    "# -------------------------\n",
    "def analyze_keywords_pytrends(keywords, max_keywords=50):\n",
    "    pytrends = TrendReq(hl='en-US', tz=330)\n",
    "    data = []\n",
    "\n",
    "    for kw in keywords[:max_keywords]:  # PyTrends works best with <50 per batch\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe='today 12-m', geo='IN')\n",
    "            time.sleep(1)  # avoid rate limit\n",
    "\n",
    "            df_trend = pytrends.interest_over_time()\n",
    "            volume = df_trend[kw].mean() if not df_trend.empty else 0\n",
    "            difficulty = round((100 - volume) / 10, 2)  # Fake metric (proxy)\n",
    "            cpc = round(volume / 10, 2)  # Fake CPC proxy (for demonstration)\n",
    "            data.append([kw, volume, difficulty, cpc])\n",
    "        except:\n",
    "            data.append([kw, 0, 0, 0])\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "# -------------------------\n",
    "# STEP 4: FINAL REPORT\n",
    "# -------------------------\n",
    "def create_final_report(url, short_tail, mid_tail, long_tail):\n",
    "    all_keywords = (\n",
    "        [(kw, \"Short-tail\") for kw in short_tail] +\n",
    "        [(kw, \"Mid-tail\") for kw in mid_tail] +\n",
    "        [(kw, \"Long-tail\") for kw in long_tail]\n",
    "    )\n",
    "    keywords_list = [kw[0] for kw in all_keywords]\n",
    "    trends_data = analyze_keywords_pytrends(keywords_list)\n",
    "\n",
    "    final_data = []\n",
    "    for (kw, tail_type), (_, volume, difficulty, cpc) in zip(all_keywords, trends_data):\n",
    "        final_data.append([kw, url, volume, difficulty, cpc, tail_type])\n",
    "\n",
    "    df = pd.DataFrame(final_data, columns=[\"Keyword\", \"Source_URL\", \"Volume\", \"Difficulty\", \"CPC\", \"Tail-Type\"])\n",
    "    df.to_excel(\"Spain Keywords.xlsx\", index=False)\n",
    "    print(\"\\nâœ… Spain Keywords.xlsx\")\n",
    "    print(df.head(10))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# STEP 5: RUNNING ALL STEPS\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://teaspoonofadventure.com/eat-in-spain/\"\n",
    "    print(f\"Scraping SEO data from: {url} ...\")\n",
    "    seo_data = scrape_seo_elements(url)\n",
    "    print(\"Title:\", seo_data['title'])\n",
    "    print(\"Meta Description:\", seo_data['meta_desc'])\n",
    "    print(\"Headings Found:\", seo_data['headings'][:5])\n",
    "    print(\"Tags:\", seo_data['tags'])\n",
    "\n",
    "    print(\"\\nExtracting keywords...\")\n",
    "    text_to_analyze = \" \".join([seo_data['title'], seo_data['meta_desc']] + seo_data['headings']) + \" \" + seo_data['body_content']\n",
    "    short_tail, mid_tail, long_tail = extract_keywords(text_to_analyze)\n",
    "\n",
    "    print(f\"Short-tail: {len(short_tail)}, Mid-tail: {len(mid_tail)}, Long-tail: {len(long_tail)}\")\n",
    "    create_final_report(url, short_tail, mid_tail, long_tail)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
